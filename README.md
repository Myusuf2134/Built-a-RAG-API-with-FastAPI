Build a RAG API with FastAPIThis project demonstrates how to build a Retrieval-Augmented Generation (RAG) API using FastAPI that can answer technical questions by combining AI with a local document knowledge base. This setup connects text embeddings, a vector database, and a local Large Language Model (LLM) to create a functional, queryable API.+1üöÄ Project OverviewThe goal of this project was to learn how APIs work by building an AI-powered service from scratch. It leverages local LLMs to ensure data privacy and performance without needing a constant internet connection or a powerful remote server.+1Key FeaturesLocal AI Inference: Uses Ollama to run models locally on your machine.Vector Search: Utilizes Chroma as a vector database to store and query document embeddings.FastAPI Framework: Efficiently handles HTTP requests, routing, and automatic documentation generation.+2Interactive Testing: Built-in Swagger UI for exploring and debugging the API directly from the browser.+1üõ†Ô∏è Tech StackServicePurposePythonThe core programming language used to build the API and AI logic.FastAPIA modern framework for building high-performance APIs.+1OllamaAn interface for running and interacting with local LLMs like TinyLlama.+2ChromaA vector database for storing and retrieving document embeddings.UvicornAn ASGI server used to run the FastAPI application.üß† How It WorksThe RAG process follows these steps to provide accurate, context-aware answers:Request: A user sends a question (e.g., "What is Kubernetes?") to the /query endpoint via a POST request.+1Embedding: The API breaks the question down into embeddings‚Äînumerical representations that capture word meaning and context.+1Retrieval: Chroma compares the question's embeddings against the document embeddings stored in the knowledge base (e.g., DevOps technical files).+1Augmentation: The most relevant document context is retrieved and passed to the TinyLlama LLM along with the original question.+1Generation: The LLM generates a clear and concise response based on the provided context.+1üíª Setup and Installation1. PrerequisitesPython: Installed on your machine.Ollama: Downloaded and running locally.TinyLlama Model: Downloaded via Ollama for lightweight, fast testing.2. Virtual EnvironmentCreate an isolated workspace to prevent package version conflicts.+1Bashpython -m venv venv
source venv/bin/activate  # On Windows use `venv\Scripts\activate`
3. Install DependenciesInstall the required Python packages.Bashpip install fastapi chromadb uvicorn ollama
4. Running the APIStart the server using Uvicorn:Bashuvicorn main:app --reload
üß™ Testing the APIVia Command Line (curl)You can test the endpoint directly from your terminal:+1Bashcurl -X 'POST' \
  'http://127.0.0.1:8000/query' \
  --data-urlencode "q=What is Kubernetes?"
